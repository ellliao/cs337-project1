{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Packages"
      ],
      "metadata": {
        "id": "8L5BEkIrDyXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGZBGqgPDhqF",
        "outputId": "a637a04e-fb78-4d59-fc38-c21d06651f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJNrm9_1Dn7_",
        "outputId": "9d451b86-9b23-4b04-b8df-dc391101f13c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3V2WJqzDn_M",
        "outputId": "fbb03792-61ff-4ee3-e9ae-69374890d683"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpaCy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iobhAYemDoFm",
        "outputId": "430a4104-e10b-497c-88e8-2b9b39ac5e45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpaCy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from SpaCy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from SpaCy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->SpaCy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SpaCy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SpaCy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SpaCy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->SpaCy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->SpaCy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->SpaCy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->SpaCy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->SpaCy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->SpaCy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->SpaCy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->SpaCy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->SpaCy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->SpaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->SpaCy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->SpaCy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->SpaCy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install blis\n",
        "%pip install langdetect\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq-hZDlpI0IE",
        "outputId": "6e43833f-ab2c-4199-9da2-0198c3bb420d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: blis in /usr/local/lib/python3.10/dist-packages (0.7.11)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from blis) (1.26.4)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=3a867a8921362d58a92bac653c9e4dbe2b6434e81f758a9ef9293cf1a829dc16\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import re\n",
        "from ftfy import fix_text\n",
        "import spacy\n",
        "import json\n",
        "from unidecode import unidecode\n",
        "from langdetect import detect, DetectorFactory\n",
        "import datetime\n",
        "import csv\n",
        "from collections import Counter\n",
        "import difflib\n",
        "from nltk.metrics import distance\n"
      ],
      "metadata": {
        "id": "8S-T4sqwIsqb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing"
      ],
      "metadata": {
        "id": "RF3_UFGGD2FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "# open write and read json file\n",
        "with open('gg2013.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Clean tweets and store RT totals\n",
        "rt_totals = {}\n",
        "for data_text in data:\n",
        "\n",
        "    # The following cleaning must be done to combine tweets properly\n",
        "    # Remove URLS\n",
        "    data_text['text'] = re.sub(r'\\A|\\shttps?://\\S+', '', data_text['text'])\n",
        "    #Fix encoding issues\n",
        "    data_text['text'] = fix_text(data_text['text'])\n",
        "    #Standardize special characters / emojis to Unicode\n",
        "    data_text['text'] = unidecode(data_text['text'])\n",
        "    #clean white space\n",
        "    data_text['text'] = \" \".join(data_text['text'].split())\n",
        "    #keep tabs/newline characters\n",
        "    data_text['text'] = re.sub(' +', ' ', data_text['text'])\n",
        "    # lowcase letters\n",
        "    # data_text['text'] = data_text['text'].lower()\n",
        "    #remove hashtags\n",
        "    data_text['text'] = re.sub(r'#', '', data_text['text'])\n",
        "\n",
        "    # Extract RT data from text\n",
        "    retweets = re.findall( \\\n",
        "        r'\\b(?:rt(?:\\+\\d+)? |\\\")@(\\w+):? (.*?)(?= // |\\\"\\Z|\\Z|\\b rt @\\w+:)', \\\n",
        "            data_text['text'], re.IGNORECASE)\n",
        "\n",
        "    # Update retweet totals\n",
        "    for rt in retweets:\n",
        "        rt = (rt[0].lower(), rt[1])\n",
        "        if rt not in rt_totals:\n",
        "            rt_totals[rt] = {'retweets': 1, 'timestamp_ms': data_text['timestamp_ms']}\n",
        "        else:\n",
        "            rt_totals[rt]['retweets'] += 1\n",
        "            rt_totals[rt]['timestamp_ms'] = \\\n",
        "                min(rt_totals[rt]['timestamp_ms'], data_text['timestamp_ms'])\n",
        "\n",
        "# Combine retweets with existing tweets\n",
        "for data_text in data:\n",
        "    original_text = data_text['text']\n",
        "\n",
        "    # Find number of retweets\n",
        "    tweet_key = (data_text['user']['screen_name'].lower(), original_text)\n",
        "    data_text['retweets'] = rt_totals.pop(tweet_key, {'retweets': 0})['retweets']\n",
        "\n",
        "# Add retweeted tweets not in original dataset\n",
        "for tweet_key in list(rt_totals):\n",
        "    tweet = {'text': tweet_key[1], \\\n",
        "             'user': {'screen_name': tweet_key[0]}, \\\n",
        "             'timestamp_ms': rt_totals[tweet_key]['timestamp_ms'], \\\n",
        "             'retweets': rt_totals[tweet_key]['retweets']}\n",
        "    data.append(tweet)\n",
        "\n",
        "preprocessed_data = []\n",
        "user_metadata = {}\n",
        "# Remove retweets and finalize preprocessed json data\n",
        "for data_text in data:\n",
        "    original_text = data_text['text']\n",
        "\n",
        "    # Update user weights based on retweet value\n",
        "    user_name = data_text['user']['screen_name'].lower()\n",
        "    if user_name not in user_metadata:\n",
        "        user_metadata[user_name] = {'num_tweets': 0, 'rt_total': 0, 'rt_average': 0}\n",
        "    user_metadata[user_name]['num_tweets'] += 1\n",
        "    user_metadata[user_name]['rt_total'] += data_text['retweets']\n",
        "    user_metadata[user_name]['rt_average'] = user_metadata[user_name]['rt_total'] \\\n",
        "        / user_metadata[user_name]['num_tweets']\n",
        "\n",
        "    # Remove retweets\n",
        "    remove_retweets = re.sub(r'(?:\\A| )(?:(?:rt(?:\\+\\d+)? )|\\\")@\\w+:?.*?\\Z|(?:\\\"\\Z)|(?: // )', \\\n",
        "                             '', original_text, 0, re.IGNORECASE)\n",
        "    # Remove empty tweets\n",
        "    if remove_retweets == '':\n",
        "        continue\n",
        "    #After pre-processing, information going back to orginal json data\n",
        "    data_text['text'] = remove_retweets\n",
        "    preprocessed_data.append(data_text)\n",
        "\n",
        "# Save data\n",
        "with open('gg2013_user_metadata.json', 'w', encoding='utf-8') as output_file:\n",
        "    json.dump(user_metadata, output_file, indent=6)\n",
        "with open('gg2013_preprocessed.json', 'w', encoding='utf-8') as output_file:\n",
        "    json.dump(preprocessed_data, output_file, indent=6)"
      ],
      "metadata": {
        "id": "fiOgxBjLDoIv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing"
      ],
      "metadata": {
        "id": "6RvzKtI_FINn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def load_processed_tweets_from_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "data = load_processed_tweets_from_json(\"gg2013_preprocessed.json\")\n",
        "def clean_retweet_text(text):\n",
        "    \"\"\"Removes the 'rt @username:' prefix from a retweet, leaving only the main content.\"\"\"\n",
        "    # Check if the text starts with 'rt @'\n",
        "    if text.startswith(\"rt @\"):\n",
        "        # Find the position of the first colon after 'rt @username'\n",
        "        colon_position = text.find(\":\")\n",
        "        # If a colon exists, return the text after it; otherwise, return the full string\n",
        "        if colon_position != -1:\n",
        "            return text[colon_position + 1:].strip()\n",
        "    return text.strip()\n",
        "\n",
        "award_name_allowlist = [\"drama\", \"musical\", \"comedy\", \"animated\", \"animation\", \"motion\", \"television\", \"series\", \"award\", \"best\"]\n",
        "\n",
        "def extract_award_name(sentence):\n",
        "    \"\"\"Extracts the award name and winner from the sentence based on specified conditions.\"\"\"\n",
        "    # Find all hyphen and colon positions\n",
        "    split_positions = [i for i, char in enumerate(sentence) if char in \"-:\"]\n",
        "    award_name = None\n",
        "    winner = None\n",
        "\n",
        "    if not re.search(r'[-:]', sentence):\n",
        "        return [None, None]  # Return None if there is no hyphen or colon\n",
        "\n",
        "        # Step 2: Check if sentence contains \"best\" (case insensitive)\n",
        "    if \"best\" not in sentence.lower():\n",
        "        return [None, None]  # Return None if \"best\" is not present\n",
        "\n",
        "    for index, pos in enumerate(split_positions):\n",
        "        # Split the sentence at the current hyphen/colon position\n",
        "\n",
        "        if index >= 1:\n",
        "            left_part = sentence[split_positions[index - 1] + 1: split_positions[index]].strip()\n",
        "        else:\n",
        "            left_part = sentence[:pos].strip()\n",
        "        right_part = sentence[pos + 1:].strip()\n",
        "\n",
        "        # Check if there's something on the right\n",
        "        if not right_part:\n",
        "            return award_name, winner\n",
        "\n",
        "        # Split left part into words\n",
        "        left_words = left_part.split()\n",
        "\n",
        "        # Check for \"best\" or \"award\" in left part\n",
        "        if any(word.lower() in left_words for word in ['best', 'award']):\n",
        "\n",
        "            # Determine the right portion based on allowlist keywords\n",
        "            if index < len(split_positions) - 1:\n",
        "                next_segment = sentence[pos + 1:split_positions[index + 1]].strip()\n",
        "            else:\n",
        "                next_segment = right_part\n",
        "\n",
        "            # Assign award name and winner based on allowlist\n",
        "            if any(word.lower() in next_segment.lower() for word in award_name_allowlist):\n",
        "                award_name = f\"{left_part} - {next_segment}\".strip()\n",
        "                # Capture the winner if more splits remain\n",
        "                if index < len(split_positions) - 2:\n",
        "                    winner = sentence[split_positions[index + 1] + 1: split_positions[index + 2]].strip()\n",
        "            else:\n",
        "                award_name = left_part\n",
        "                winner = next_segment\n",
        "            break\n",
        "    if winner and '.' in winner:\n",
        "        winner = re.split(r'\\.', winner, 1)[0].strip()\n",
        "\n",
        "    return [award_name, winner]\n",
        "def extract_winner_info(text):\n",
        "    \"\"\"Check if the sentence contains a pattern like '... wins ...' with 'best' or 'award' in the second part.\"\"\"\n",
        "    text = clean_retweet_text(text)\n",
        "\n",
        "    pattern = r'^(.*?)(wins|receives)(.*?)(best)(.*?)$'\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    # Check each sentence individually\n",
        "    for sentence in sentences:\n",
        "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            # Extract parts based on the match\n",
        "            first_part = match.group(1).strip()\n",
        "            second_part = match.group(3).strip() + \" \" + match.group(4).strip() + \" \" + match.group(5).strip()\n",
        "\n",
        "            return [first_part, second_part]\n",
        "\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_award_sequences(input_str):\n",
        "\n",
        "    initial_pattern = r\"(?i)^(?:golden globes for|golden globes|award for)\\s+(best)\\s+(.+)\"\n",
        "    match = re.search(initial_pattern, input_str)\n",
        "    if match:\n",
        "        input_str = match.group(1) + \" \" + match.group(2)\n",
        "\n",
        "\n",
        "\n",
        "    refine_pattern = r\"(?i)(best|award)\\s+(.+?)(?:\\s+golden globes|goldenglobes|$)\"\n",
        "    match = re.search(refine_pattern, input_str)\n",
        "    if match:\n",
        "        input_str = match.group(1) + \" \" + match.group(2).strip()\n",
        "\n",
        "\n",
        "    split_parts = re.split(r'(\\s*-\\s*|\\s+for\\s+)', input_str)\n",
        "    sequences = []\n",
        "    current_sequence = split_parts[0].strip()\n",
        "\n",
        "\n",
        "    for i in range(1, len(split_parts) - 1, 2):\n",
        "        sequences.append(current_sequence)\n",
        "        separator = split_parts[i].strip()\n",
        "        next_part = split_parts[i + 1].strip()\n",
        "        current_sequence += f\" {separator} {next_part}\"\n",
        "\n",
        "    sequences.append(current_sequence)\n",
        "\n",
        "    return sequences\n",
        "for index, tweet in enumerate(data):\n",
        "\n",
        "    tweet_text = tweet['text']\n",
        "    tweet_text = clean_retweet_text(tweet_text)\n",
        "\n",
        "    award_name = extract_winner_info(tweet_text)\n",
        "\n",
        "    if award_name:\n",
        "        win_resolutions = {\n",
        "            \"award\" : extract_award_sequences(award_name[1]),\n",
        "            \"winner\": [award_name[0]],\n",
        "            \"confidence\": 0.7\n",
        "\n",
        "        }\n",
        "        tweet[\"win_resolutions\"] = win_resolutions\n",
        "\n",
        "        continue\n",
        "    award_name = extract_award_name(tweet_text)\n",
        "\n",
        "    if award_name[0]:\n",
        "        win_resolutions = {\n",
        "            \"award\" : extract_award_sequences(award_name[0]),\n",
        "            \"winner\": [award_name[1]],\n",
        "            \"confidence\": 0.8\n",
        "\n",
        "        }\n",
        "        tweet[\"win_resolutions\"] = win_resolutions"
      ],
      "metadata": {
        "id": "2WYFtQ0JJK_e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def find_people_from_text(text):\n",
        "\n",
        "    # Skip if not English\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return {}\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "    doc = nlp(text)\n",
        "    confidence = 0.5\n",
        "\n",
        "    # Find the people mentioned in the tweet\n",
        "    potential_hosts = [entity for entity in doc.ents \\\n",
        "                           if entity.label_ == \"PERSON\"]\n",
        "    # Add to hosts\n",
        "    hosts =[host.text for host in potential_hosts]\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "\n",
        "        # Find the root verb of the sentence\n",
        "        root_verb = [token for token in sentence if token.dep_ == \"ROOT\"]\n",
        "\n",
        "        if root_verb and root_verb[0].lemma_ == \"host\":\n",
        "            confidence = 0.7\n",
        "            break\n",
        "\n",
        "    return {'hosts': hosts, 'confidence': confidence}\n",
        "have_host_resolutions = []\n",
        "for index, tweet in enumerate(data):\n",
        "\n",
        "    tweet_text = tweet['text']\n",
        "\n",
        "    # Skip if no mention of host in text, or about next year\n",
        "    if re.search(r'(?:\\bhost)|(?:host(?:s|ed|ing)?\\b)', tweet_text, re.IGNORECASE) == None \\\n",
        "        or re.search(r'\\bnext\\b', tweet_text, re.IGNORECASE) != None:\n",
        "        host_resolutions = {}\n",
        "    else:\n",
        "        host_resolutions = find_people_from_text(tweet_text)\n",
        "\n",
        "    if host_resolutions:\n",
        "        have_host_resolutions.append(index)\n",
        "        tweet[\"host_resolutions\"] = host_resolutions\n",
        "def split_string_on_and(text: str):\n",
        "    # Split the text by \"and\" or \"&\" with surrounding whitespace handling\n",
        "    parts = re.split(r'\\s*(?:\\band\\b|&)\\s*', text)\n",
        "    # Filter out any empty strings in the list\n",
        "    return [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "def parse_presenters(tweet: str):\n",
        "    # Define the regex pattern to match the presenters and award pattern\n",
        "    pattern = r\"(.*?)(?<!re)(?:\\bpresent\\b|\\bpresents\\b|\\bpresenting\\b|\\bpresenting for\\b)\\s+(.*?(?:best).*)\"\n",
        "\n",
        "    # Split the tweet into sentences\n",
        "    sentences = re.split(r'[.!?]', tweet)\n",
        "\n",
        "    # Check each sentence individually\n",
        "    for sentence in sentences:\n",
        "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            # Extract the presenters and award from the match\n",
        "            presenters = match.group(1).strip()\n",
        "            award = match.group(2).strip()\n",
        "\n",
        "            # Clean the award string\n",
        "            # Remove any words before \"best\"\n",
        "            award = re.sub(r\".*?\\bbest\\b\", \"best\", award, flags=re.IGNORECASE)\n",
        "            # Remove any words including and after \"at,\" \"and,\" \"for,\" or \"to\"\n",
        "            award = re.sub(r\"\\s+\\b(at|and|for|to)\\b.*\", \"\", award, flags=re.IGNORECASE)\n",
        "\n",
        "            return [award.strip(),   split_string_on_and(presenters)]\n",
        "\n",
        "\n",
        "    return [None, None]\n",
        "for index, tweet in enumerate(data):\n",
        "\n",
        "    tweet_text = tweet['text']\n",
        "    tweet_text = clean_retweet_text(tweet_text)\n",
        "\n",
        "\n",
        "    possible_presenters  = parse_presenters(tweet_text)\n",
        "    if possible_presenters[0]:\n",
        "      present_resolutions = {\"award\": possible_presenters[0], \"presenters\": possible_presenters[1], \"confidence\": 0.7}\n",
        "      tweet[\"present_resolutions\"] = present_resolutions\n",
        "\n",
        "\n",
        "def get_nominees(text: str):\n",
        "    # Define the regex pattern to match \"nominee/nominated/nominees ... ... best\"\n",
        "    pattern = r\"(.*)\\b(?:nominee|nominated|nominees)\\b(.*?\\bbest\\b.*)\"\n",
        "    # Define a list of exclusion words/phrases\n",
        "    exclusions = r\"\\b(?:not|should've|should have|wasn't|introduce|present|should)\\b\"\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    # Check each sentence individually\n",
        "    for sentence in sentences:\n",
        "        # Check if any exclusion word is present\n",
        "        if re.search(exclusions, sentence, re.IGNORECASE):\n",
        "            continue\n",
        "\n",
        "\n",
        "        match = re.search(pattern, sentence, re.IGNORECASE)\n",
        "        if match:\n",
        "            # Clean and separate the nominee part and award name\n",
        "            before_nominee = re.sub(r\"\\b(?:nominee|nominated|nominees)\\b\", \"\", match.group(1), flags=re.IGNORECASE).strip()\n",
        "            award_name = re.sub(r\".*?\\bbest\\b\", \"best\", match.group(2), flags=re.IGNORECASE).strip()\n",
        "\n",
        "            return [before_nominee, award_name]\n",
        "\n",
        "\n",
        "    return None\n",
        "def get_nominees2(text: str) :\n",
        "    # Define the regex pattern to match a sentence with a negative word, \"win/won,\" and \"best\"\n",
        "    pattern = r\"(.*?)(\\bdoes not\\b|\\bnot\\b|\\bshould have\\b|\\bshould've\\b|\\bshould\\b|\\bdidn't\\b|\\bdid not\\b)(.*?\\b(?:win|won)\\b.*?\\bbest\\b.*)\"\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'[.:!?]', text)\n",
        "\n",
        "    # Process each sentence individually\n",
        "    for sentence in sentences:\n",
        "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            # Extract the part before the negative word as the nominee\n",
        "            nominee = match.group(1).strip()\n",
        "            # Extract the part from \"best\" onward as the award_name\n",
        "            award_name = re.sub(r\".*?\\bbest\\b\", \"best\", match.group(3), flags=re.IGNORECASE).strip()\n",
        "\n",
        "            # Clean the nominee by removing text before and including \"that\" or \"if\"\n",
        "            nominee = re.sub(r\".*\\b(that|if)\\b\", \"\", nominee, flags=re.IGNORECASE).strip()\n",
        "\n",
        "            return [nominee, award_name]\n",
        "\n",
        "\n",
        "    return None\n",
        "for index, tweet in enumerate(data):\n",
        "\n",
        "    tweet_text = tweet['text']\n",
        "    tweet_text = clean_retweet_text(tweet_text)\n",
        "\n",
        "\n",
        "    possible_nominee  = get_nominees(tweet_text)\n",
        "    if possible_nominee:\n",
        "\n",
        "\n",
        "        nominee_resolutions = {\"award\": possible_nominee[1], \"nominee\": possible_nominee[0], \"confidence\": 0.5}\n",
        "        tweet[\"nominee_resolutions\"] = nominee_resolutions\n",
        "\n",
        "    possible_nominee  = get_nominees2(tweet_text)\n",
        "    if possible_nominee:\n",
        "\n",
        "\n",
        "        nominee_resolutions = {\"award\": possible_nominee[1], \"nominee\": possible_nominee[0], \"confidence\": 0.5}\n",
        "        tweet[\"nominee_resolutions\"] = nominee_resolutions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "filename = 'parsed_data.json'\n",
        "\n",
        "# Write data to JSON file\n",
        "with open(filename, 'w') as json_file:\n",
        "    json.dump(data, json_file, indent=4)"
      ],
      "metadata": {
        "id": "lhDXrlAqD4i9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregation"
      ],
      "metadata": {
        "id": "6rYyLHs5ECZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read extracted tweets\n",
        "with open('parsed_data.json', 'r', encoding='utf-8') as extracted_file:\n",
        "    extracted_data = json.load(extracted_file)\n",
        "# Open and read user metadata\n",
        "with open('gg2013_user_metadata.json', 'r', encoding='utf-8') as metadata_file:\n",
        "    user_metadata = json.load(metadata_file)\n",
        "\n",
        "# Find ceremony year\n",
        "dt = datetime.datetime.fromtimestamp(extracted_data[0]['timestamp_ms'] / 1000.0,\n",
        "                                     tz=datetime.timezone.utc)\n",
        "year = dt.year\n",
        "\n",
        "# Import IMDB data relevant to the ceremony year\n",
        "def get_imdb_title_basics():\n",
        "\n",
        "    title_basics = {}\n",
        "\n",
        "    with open('./imdb/title.basics.tsv') as imdb_file:\n",
        "        reader = csv.DictReader(imdb_file, delimiter='\\t', quotechar='\"')\n",
        "        for row in reader:\n",
        "            # Skip title with unknown start years\n",
        "            if row['startYear'] == \"\\\\N\":\n",
        "                continue\n",
        "\n",
        "            # Save title if it is from the current or previous year, OR if it is a\n",
        "            # TV series that ran at some point during that time, AND it is not an\n",
        "            # episode or special\n",
        "            start_year = int(row['startYear'])\n",
        "            if ((row['endYear'] == \"\\\\N\" \\\n",
        "                 and (start_year == year or start_year == year-1 \\\n",
        "                    or (row['titleType'] == \"tvSeries\" and start_year < year))) or \\\n",
        "                (row['endYear'] != \"\\\\N\" and start_year <= year \\\n",
        "                 and int(row['endYear']) >= year-1)) and \\\n",
        "               row['titleType'] != \"tvEpisode\" and \\\n",
        "               row['titleType'] != 'tvSpecial':\n",
        "                category = row['titleType'].lower()\n",
        "                if category not in title_basics:\n",
        "                    title_basics[category] = []\n",
        "                title = row['primaryTitle']\n",
        "                title_basics[category].append(title)\n",
        "\n",
        "    return title_basics\n",
        "title_basics = get_imdb_title_basics()\n",
        "# IMDB genre tags, minus 'Short' and 'Adult'\n",
        "genres = ['Comedy', 'Music', 'Crime', 'Drama', 'Game-Show', 'Talk-Show', 'Family', 'Mystery', 'Sport', 'Horror', 'Western', 'Adventure', 'News', 'Action', 'Documentary', 'Reality-TV', 'Sci-Fi', 'Thriller', 'Animation', 'War', 'Musical', 'Romance', 'Fantasy', 'Biography', 'History']\n",
        "# Set up results object\n",
        "results = {}\n",
        "\n",
        "# Ranking\n",
        "possible_hosts = Counter()\n",
        "possible_awards = Counter()\n",
        "for tweet in extracted_data:\n",
        "    # Calculate tweet weight based on user + tweet reliability\n",
        "    user = tweet['user']['screen_name'].lower()\n",
        "    tweet['weight'] = (tweet['retweets'] + 1) \\\n",
        "        + (user_metadata[user]['rt_average'] + 1)\n",
        "\n",
        "    # Host\n",
        "    if 'host_resolutions' in tweet:\n",
        "        hr = tweet['host_resolutions']\n",
        "        for host in hr['hosts']:\n",
        "            possible_hosts[host.lower()] += tweet['weight'] * hr['confidence']\n",
        "\n",
        "    # Award\n",
        "    if 'win_resolutions' in tweet:\n",
        "        wr = tweet['win_resolutions']\n",
        "        for award in wr['award']:\n",
        "            possible_awards[award.lower()] += tweet['weight'] * wr['confidence']\n",
        "    if 'nominee_resolutions' in tweet:\n",
        "        nr = tweet['nominee_resolutions']\n",
        "        possible_awards[nr['award'].lower()] += tweet['weight'] * nr['confidence']\n",
        "    if 'present_resolutions' in tweet:\n",
        "        pr = tweet['present_resolutions']\n",
        "        possible_awards[pr['award'].lower()] += tweet['weight'] * pr['confidence']\n",
        "\n",
        "\n",
        "# Find closest match by averaging distance calculations\n",
        "def closest_match(names, possible_names, diff):\n",
        "    if not isinstance(names, list):\n",
        "        names = [names]\n",
        "\n",
        "    max_ratio = []\n",
        "    for name in names:\n",
        "        name = name.lower()\n",
        "        sm = difflib.SequenceMatcher(b=name)\n",
        "        for other_name in possible_names:\n",
        "            other_name = other_name.lower()\n",
        "            ratio = 1 - distance.edit_distance(name, other_name) / \\\n",
        "                max(len(name), len(other_name))\n",
        "            sm.set_seq1(other_name)\n",
        "            ratio = (sm.ratio() + ratio)/2.0\n",
        "            if not max_ratio or (max_ratio and ratio > max_ratio[1]):\n",
        "                max_ratio = [other_name, ratio]\n",
        "\n",
        "    if max_ratio and max_ratio[1] > diff:\n",
        "        return max_ratio[0]\n",
        "    return []\n",
        "\n",
        "# Combine like name categories\n",
        "def combine_names(name, possible_names, orig_vals, len_penalty, diff):\n",
        "    if name not in possible_names:\n",
        "         return\n",
        "\n",
        "    # Temporarily remove current name to avoid comparing it to itself\n",
        "    temp = possible_names.pop(name)\n",
        "\n",
        "    # Check for direct substrings\n",
        "    for other_name in possible_names:\n",
        "        # Prioritize longer string, unless large popularity difference\n",
        "        if (name in other_name \\\n",
        "            and orig_vals[name] * len_penalty < orig_vals[other_name]) \\\n",
        "                or (other_name in name \\\n",
        "                    and orig_vals[other_name] * len_penalty >= orig_vals[name]):\n",
        "                possible_names[other_name] += temp\n",
        "                return\n",
        "        elif name in other_name or other_name in name:\n",
        "            possible_names[name] = temp + possible_names[other_name]\n",
        "            del possible_names[other_name]\n",
        "            return\n",
        "\n",
        "    # If not a direct substring, check for misspellings\n",
        "    closest = closest_match(name, possible_names.keys(), diff)\n",
        "    if closest and orig_vals[name] > orig_vals[closest]:\n",
        "        possible_names[name] = temp + possible_names[closest]\n",
        "        del possible_names[closest]\n",
        "        return\n",
        "    elif closest:\n",
        "        possible_names[closest] += temp\n",
        "        return\n",
        "\n",
        "    # Reinsert current name if it is distinct\n",
        "    possible_names[name] = temp\n",
        "\n",
        "# Retrieve the most likely combination of names from a given set\n",
        "def get_names(possible_names, threshold):\n",
        "    names = []\n",
        "    if not possible_names:\n",
        "        return names\n",
        "\n",
        "    # Set the minimum score based on the given threshold\n",
        "    min_score = threshold\n",
        "    if threshold <= 1:\n",
        "        min_score *= possible_names[0][1]\n",
        "    for name in possible_names:\n",
        "        # Insert all possible hosts above the minimum score\n",
        "        if name[1] >= min_score:\n",
        "            names.append(name[0])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return names\n",
        "\n",
        "# Aggregate results for a category\n",
        "def aggregate_results(possible_options, len_penalty, diff, threshold):\n",
        "    in_order = possible_options.most_common()\n",
        "    in_order.reverse()\n",
        "    orig_vals = dict(in_order)\n",
        "    for name in orig_vals:\n",
        "        combine_names(name, possible_options, orig_vals, len_penalty, diff)\n",
        "    return get_names(possible_options.most_common(), threshold)\n",
        "## Aggregation\n",
        "\n",
        "# Hosts\n",
        "results['hosts'] = aggregate_results(possible_hosts, 0.8, 0.8, 0.8)\n",
        "\n",
        "# Award names\n",
        "awards = aggregate_results(possible_awards, 0.8, 0.7, 10)\n",
        "for award in awards:\n",
        "    results[award] = {}\n",
        "# Per-award ranking\n",
        "possible_winners = {}\n",
        "possible_nominees = {}\n",
        "possible_presenters = {}\n",
        "\n",
        "for award in awards:\n",
        "    possible_winners[award] = Counter()\n",
        "    possible_nominees[award] = Counter()\n",
        "    possible_presenters[award] = Counter()\n",
        "\n",
        "for tweet in extracted_data:\n",
        "\n",
        "    # Winner\n",
        "    if 'win_resolutions' in tweet:\n",
        "        wr = tweet['win_resolutions']\n",
        "        award = closest_match(wr['award'], awards, 0.8)\n",
        "        if award:\n",
        "            for winner in wr['winner']:\n",
        "                if winner:\n",
        "                    possible_winners[award][winner.lower()] += \\\n",
        "                        tweet['weight'] * wr['confidence']\n",
        "\n",
        "    # Nominee\n",
        "    if 'nominee_resolutions' in tweet:\n",
        "        nr = tweet['nominee_resolutions']\n",
        "        award = closest_match(nr['award'], awards, 0.8)\n",
        "        if award and nr['nominee']:\n",
        "            possible_nominees[award][nr['nominee'].lower()] += \\\n",
        "                tweet['weight'] * nr['confidence']\n",
        "\n",
        "    # Presenter\n",
        "    if 'present_resolutions' in tweet:\n",
        "        pr = tweet['present_resolutions']\n",
        "        award = closest_match(pr['award'], awards, 0.8)\n",
        "        if award:\n",
        "            for presenter in pr['presenters']:\n",
        "                if presenter:\n",
        "                    possible_presenters[award][presenter.lower()] += \\\n",
        "                        tweet['weight'] * pr['confidence']\n",
        "# Per-award aggregation\n",
        "win_penalty = 0.1\n",
        "for award in awards:\n",
        "    winner = aggregate_results(possible_winners[award], 0.8, 0.8, 1)\n",
        "    if winner:\n",
        "        results[award]['winner'] = winner[0]\n",
        "        # Insert award winner into possible nominees\n",
        "        possible_nominees[award][winner[0]] = \\\n",
        "            win_penalty * possible_winners[award][winner[0]]\n",
        "    else:\n",
        "        results[award]['winner'] = ''\n",
        "    results[award]['nominees'] = \\\n",
        "        aggregate_results(possible_nominees[award], 0.8, 0.8, 0.2)\n",
        "    results[award]['presenters'] = \\\n",
        "        aggregate_results(possible_presenters[award], 0.8, 0.8, 0.4)\n",
        "######Humor: Addtional goal\n",
        "humor = ['guffaw', 'fun', 'smirk', 'yelp', 'wag', 'lol', 'funny', 'wry', 'sneer', 'joke']\n",
        "#store data in list\n",
        "humor_list = []\n",
        "for t in extracted_data:\n",
        "    for x in humor:\n",
        "        if x in t['text']:\n",
        "            humor_list.append((t['user']['screen_name'], t['text']))\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "# Output to JSON\n",
        "with open('gg2013_results.json', 'w', encoding='utf-8') as output_file:\n",
        "    json.dump(results, output_file, indent=6)\n",
        "# Output human-readable to TXT\n",
        "with open('gg2013_results.txt', 'w', encoding='utf-8') as output_file:\n",
        "    print('Hosts:', (', '.join(results['hosts']).title()), file=output_file)\n",
        "    for award in awards:\n",
        "        print(\"\\nAward: \", award.title(), sep='', file=output_file)\n",
        "        for category in results[award]:\n",
        "            if isinstance(results[award][category], list):\n",
        "                print(category.title(), \": \",\n",
        "                      (', '.join(results[award][category])).title(), sep='',\n",
        "                      file=output_file)\n",
        "            else:\n",
        "                print(category.title(), \": \", results[award][category].title(),\n",
        "                      sep='', file=output_file)\n",
        "    for y in humor_list:\n",
        "        print(f\"Humor_Who: {y[0]}, Best Jokes: {y[1]}\", file=output_file)"
      ],
      "metadata": {
        "id": "Deqf9giXEF8C"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}