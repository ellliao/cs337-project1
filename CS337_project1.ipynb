{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L5BEkIrDyXi"
   },
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8S-T4sqwIsqb"
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "import spacy\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "from langdetect import detect, DetectorFactory\n",
    "import datetime\n",
    "import csv\n",
    "from collections import Counter\n",
    "import difflib\n",
    "from nltk.metrics import distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = input(\"Enter input JSON file path: \")\n",
    "fname = fpath.removesuffix('.json')\n",
    "\n",
    "# open write and read json file\n",
    "with open(fpath, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF3_UFGGD2FY"
   },
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fiOgxBjLDoIv"
   },
   "outputs": [],
   "source": [
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Clean tweets and store RT totals\n",
    "rt_totals = {}\n",
    "for data_text in data:\n",
    "\n",
    "    # The following cleaning must be done to combine tweets properly\n",
    "    # Remove URLS\n",
    "    data_text['text'] = re.sub(r'\\A|\\shttps?://\\S+', '', data_text['text'])\n",
    "    #Fix encoding issues\n",
    "    data_text['text'] = fix_text(data_text['text'])\n",
    "    #Standardize special characters / emojis to Unicode\n",
    "    data_text['text'] = unidecode(data_text['text'])\n",
    "    #clean white space\n",
    "    data_text['text'] = \" \".join(data_text['text'].split())\n",
    "    #keep tabs/newline characters\n",
    "    data_text['text'] = re.sub(' +', ' ', data_text['text'])\n",
    "    # lowcase letters\n",
    "    # data_text['text'] = data_text['text'].lower()\n",
    "    #remove hashtags\n",
    "    data_text['text'] = re.sub(r'#', '', data_text['text'])\n",
    "\n",
    "    # Extract RT data from text\n",
    "    retweets = re.findall( \\\n",
    "        r'\\b(?:rt(?:\\+\\d+)? |\\\")@(\\w+):? (.*?)(?= // |\\\"\\Z|\\Z|\\b rt @\\w+:)', \\\n",
    "            data_text['text'], re.IGNORECASE)\n",
    "\n",
    "    # Update retweet totals\n",
    "    for rt in retweets:\n",
    "        rt = (rt[0].lower(), rt[1])\n",
    "        if rt not in rt_totals:\n",
    "            rt_totals[rt] = {'retweets': 1, 'timestamp_ms': data_text['timestamp_ms']}\n",
    "        else:\n",
    "            rt_totals[rt]['retweets'] += 1\n",
    "            rt_totals[rt]['timestamp_ms'] = \\\n",
    "                min(rt_totals[rt]['timestamp_ms'], data_text['timestamp_ms'])\n",
    "\n",
    "# Combine retweets with existing tweets\n",
    "for data_text in data:\n",
    "    original_text = data_text['text']\n",
    "\n",
    "    # Find number of retweets\n",
    "    tweet_key = (data_text['user']['screen_name'].lower(), original_text)\n",
    "    data_text['retweets'] = rt_totals.pop(tweet_key, {'retweets': 0})['retweets']\n",
    "\n",
    "# Add retweeted tweets not in original dataset\n",
    "for tweet_key in list(rt_totals):\n",
    "    tweet = {'text': tweet_key[1], \\\n",
    "             'user': {'screen_name': tweet_key[0]}, \\\n",
    "             'timestamp_ms': rt_totals[tweet_key]['timestamp_ms'], \\\n",
    "             'retweets': rt_totals[tweet_key]['retweets']}\n",
    "    data.append(tweet)\n",
    "\n",
    "preprocessed_data = []\n",
    "user_metadata = {}\n",
    "# Remove retweets and finalize preprocessed json data\n",
    "for data_text in data:\n",
    "    original_text = data_text['text']\n",
    "\n",
    "    # Update user weights based on retweet value\n",
    "    user_name = data_text['user']['screen_name'].lower()\n",
    "    if user_name not in user_metadata:\n",
    "        user_metadata[user_name] = {'num_tweets': 0, 'rt_total': 0, 'rt_average': 0}\n",
    "    user_metadata[user_name]['num_tweets'] += 1\n",
    "    user_metadata[user_name]['rt_total'] += data_text['retweets']\n",
    "    user_metadata[user_name]['rt_average'] = user_metadata[user_name]['rt_total'] \\\n",
    "        / user_metadata[user_name]['num_tweets']\n",
    "\n",
    "    # Remove retweets\n",
    "    remove_retweets = re.sub(r'(?:\\A| )(?:(?:rt(?:\\+\\d+)? )|\\\")@\\w+:?.*?\\Z|(?:\\\"\\Z)|(?: // )', \\\n",
    "                             '', original_text, 0, re.IGNORECASE)\n",
    "    # Remove empty tweets\n",
    "    if remove_retweets == '':\n",
    "        continue\n",
    "    #After pre-processing, information going back to orginal json data\n",
    "    data_text['text'] = remove_retweets\n",
    "    preprocessed_data.append(data_text)\n",
    "\n",
    "# Save data\n",
    "with open('_'.join([fname, 'user_metadata.json']), 'w', encoding='utf-8') \\\n",
    "    as output_file:\n",
    "    json.dump(user_metadata, output_file, indent=6)\n",
    "with open('_'.join([fname, 'preprocessed.json']), 'w', encoding='utf-8') \\\n",
    "    as output_file:\n",
    "    json.dump(preprocessed_data, output_file, indent=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RvzKtI_FINn"
   },
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2WYFtQ0JJK_e"
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def load_processed_tweets_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "data = load_processed_tweets_from_json('_'.join([fname, \"preprocessed.json\"]))\n",
    "def clean_retweet_text(text):\n",
    "    \"\"\"Removes the 'rt @username:' prefix from a retweet, leaving only the main content.\"\"\"\n",
    "    # Check if the text starts with 'rt @'\n",
    "    if text.startswith(\"rt @\"):\n",
    "        # Find the position of the first colon after 'rt @username'\n",
    "        colon_position = text.find(\":\")\n",
    "        # If a colon exists, return the text after it; otherwise, return the full string\n",
    "        if colon_position != -1:\n",
    "            return text[colon_position + 1:].strip()\n",
    "    return text.strip()\n",
    "\n",
    "award_name_allowlist = [\"drama\", \"musical\", \"comedy\", \"animated\", \"animation\", \"motion\", \"television\", \"series\", \"award\", \"best\"]\n",
    "\n",
    "def extract_award_name(sentence):\n",
    "    \"\"\"Extracts the award name and winner from the sentence based on specified conditions.\"\"\"\n",
    "    # Find all hyphen and colon positions\n",
    "    split_positions = [i for i, char in enumerate(sentence) if char in \"-:\"]\n",
    "    award_name = None\n",
    "    winner = None\n",
    "\n",
    "    if not re.search(r'[-:]', sentence):\n",
    "        return [None, None]  # Return None if there is no hyphen or colon\n",
    "\n",
    "        # Step 2: Check if sentence contains \"best\" (case insensitive)\n",
    "    if \"best\" not in sentence.lower():\n",
    "        return [None, None]  # Return None if \"best\" is not present\n",
    "\n",
    "    for index, pos in enumerate(split_positions):\n",
    "        # Split the sentence at the current hyphen/colon position\n",
    "\n",
    "        if index >= 1:\n",
    "            left_part = sentence[split_positions[index - 1] + 1: split_positions[index]].strip()\n",
    "        else:\n",
    "            left_part = sentence[:pos].strip()\n",
    "        right_part = sentence[pos + 1:].strip()\n",
    "\n",
    "        # Check if there's something on the right\n",
    "        if not right_part:\n",
    "            return award_name, winner\n",
    "\n",
    "        # Split left part into words\n",
    "        left_words = left_part.split()\n",
    "\n",
    "        # Check for \"best\" or \"award\" in left part\n",
    "        if any(word.lower() in left_words for word in ['best', 'award']):\n",
    "\n",
    "            # Determine the right portion based on allowlist keywords\n",
    "            if index < len(split_positions) - 1:\n",
    "                next_segment = sentence[pos + 1:split_positions[index + 1]].strip()\n",
    "            else:\n",
    "                next_segment = right_part\n",
    "\n",
    "            # Assign award name and winner based on allowlist\n",
    "            if any(word.lower() in next_segment.lower() for word in award_name_allowlist):\n",
    "                award_name = f\"{left_part} - {next_segment}\".strip()\n",
    "                # Capture the winner if more splits remain\n",
    "                if index < len(split_positions) - 2:\n",
    "                    winner = sentence[split_positions[index + 1] + 1: split_positions[index + 2]].strip()\n",
    "            else:\n",
    "                award_name = left_part\n",
    "                winner = next_segment\n",
    "            break\n",
    "    if winner and '.' in winner:\n",
    "        winner = re.split(r'\\.', winner, 1)[0].strip()\n",
    "\n",
    "    return [award_name, winner]\n",
    "def extract_winner_info(text):\n",
    "    \"\"\"Check if the sentence contains a pattern like '... wins ...' with 'best' or 'award' in the second part.\"\"\"\n",
    "    text = clean_retweet_text(text)\n",
    "\n",
    "    pattern = r'^(.*?)(wins|receives)(.*?)(best)(.*?)$'\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "\n",
    "    # Check each sentence individually\n",
    "    for sentence in sentences:\n",
    "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            # Extract parts based on the match\n",
    "            first_part = match.group(1).strip()\n",
    "            second_part = match.group(3).strip() + \" \" + match.group(4).strip() + \" \" + match.group(5).strip()\n",
    "\n",
    "            return [first_part, second_part]\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_award_sequences(input_str):\n",
    "\n",
    "    initial_pattern = r\"(?i)^(?:golden globes for|golden globes|award for)\\s+(best)\\s+(.+)\"\n",
    "    match = re.search(initial_pattern, input_str)\n",
    "    if match:\n",
    "        input_str = match.group(1) + \" \" + match.group(2)\n",
    "\n",
    "\n",
    "\n",
    "    refine_pattern = r\"(?i)(best|award)\\s+(.+?)(?:\\s+golden globes|goldenglobes|$)\"\n",
    "    match = re.search(refine_pattern, input_str)\n",
    "    if match:\n",
    "        input_str = match.group(1) + \" \" + match.group(2).strip()\n",
    "\n",
    "\n",
    "    split_parts = re.split(r'(\\s*-\\s*|\\s+for\\s+)', input_str)\n",
    "    sequences = []\n",
    "    current_sequence = split_parts[0].strip()\n",
    "\n",
    "\n",
    "    for i in range(1, len(split_parts) - 1, 2):\n",
    "        sequences.append(current_sequence)\n",
    "        separator = split_parts[i].strip()\n",
    "        next_part = split_parts[i + 1].strip()\n",
    "        current_sequence += f\" {separator} {next_part}\"\n",
    "\n",
    "    sequences.append(current_sequence)\n",
    "\n",
    "    return sequences\n",
    "for index, tweet in enumerate(data):\n",
    "\n",
    "    tweet_text = tweet['text']\n",
    "    tweet_text = clean_retweet_text(tweet_text)\n",
    "\n",
    "    award_name = extract_winner_info(tweet_text)\n",
    "\n",
    "    if award_name:\n",
    "        win_resolutions = {\n",
    "            \"award\" : extract_award_sequences(award_name[1]),\n",
    "            \"winner\": [award_name[0]],\n",
    "            \"confidence\": 0.7\n",
    "\n",
    "        }\n",
    "        tweet[\"win_resolutions\"] = win_resolutions\n",
    "\n",
    "        continue\n",
    "    award_name = extract_award_name(tweet_text)\n",
    "\n",
    "    if award_name[0]:\n",
    "        win_resolutions = {\n",
    "            \"award\" : extract_award_sequences(award_name[0]),\n",
    "            \"winner\": [award_name[1]],\n",
    "            \"confidence\": 0.8\n",
    "\n",
    "        }\n",
    "        tweet[\"win_resolutions\"] = win_resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhDXrlAqD4i9"
   },
   "outputs": [],
   "source": [
    "nlp = spacy_model\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def find_people_from_text(text):\n",
    "\n",
    "    # Skip if not English\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            return {}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    doc = nlp(text)\n",
    "    confidence = 0.5\n",
    "\n",
    "    # Find the people mentioned in the tweet\n",
    "    potential_hosts = [entity for entity in doc.ents \\\n",
    "                           if entity.label_ == \"PERSON\"]\n",
    "    # Add to hosts\n",
    "    hosts =[host.text for host in potential_hosts]\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "\n",
    "        # Find the root verb of the sentence\n",
    "        root_verb = [token for token in sentence if token.dep_ == \"ROOT\"]\n",
    "\n",
    "        if root_verb and root_verb[0].lemma_ == \"host\":\n",
    "            confidence = 0.7\n",
    "            break\n",
    "\n",
    "    return {'hosts': hosts, 'confidence': confidence}\n",
    "have_host_resolutions = []\n",
    "for index, tweet in enumerate(data):\n",
    "\n",
    "    tweet_text = tweet['text']\n",
    "\n",
    "    # Skip if no mention of host in text, or about next year\n",
    "    if re.search(r'(?:\\bhost)|(?:host(?:s|ed|ing)?\\b)', tweet_text, re.IGNORECASE) == None \\\n",
    "        or re.search(r'\\bnext\\b', tweet_text, re.IGNORECASE) != None:\n",
    "        host_resolutions = {}\n",
    "    else:\n",
    "        host_resolutions = find_people_from_text(tweet_text)\n",
    "\n",
    "    if host_resolutions:\n",
    "        have_host_resolutions.append(index)\n",
    "        tweet[\"host_resolutions\"] = host_resolutions\n",
    "def split_string_on_and(text: str):\n",
    "    # Split the text by \"and\" or \"&\" with surrounding whitespace handling\n",
    "    parts = re.split(r'\\s*(?:\\band\\b|&)\\s*', text)\n",
    "    # Filter out any empty strings in the list\n",
    "    return [part.strip() for part in parts if part.strip()]\n",
    "\n",
    "def parse_presenters(tweet: str):\n",
    "    # Define the regex pattern to match the presenters and award pattern\n",
    "    pattern = r\"(.*?)(?<!re)(?:\\bpresent\\b|\\bpresents\\b|\\bpresenting\\b|\\bpresenting for\\b)\\s+(.*?(?:best).*)\"\n",
    "\n",
    "    # Split the tweet into sentences\n",
    "    sentences = re.split(r'[.!?]', tweet)\n",
    "\n",
    "    # Check each sentence individually\n",
    "    for sentence in sentences:\n",
    "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            # Extract the presenters and award from the match\n",
    "            presenters = match.group(1).strip()\n",
    "            award = match.group(2).strip()\n",
    "\n",
    "            # Clean the award string\n",
    "            # Remove any words before \"best\"\n",
    "            award = re.sub(r\".*?\\bbest\\b\", \"best\", award, flags=re.IGNORECASE)\n",
    "            # Remove any words including and after \"at,\" \"and,\" \"for,\" or \"to\"\n",
    "            award = re.sub(r\"\\s+\\b(at|and|for|to)\\b.*\", \"\", award, flags=re.IGNORECASE)\n",
    "\n",
    "            return [award.strip(),   split_string_on_and(presenters)]\n",
    "\n",
    "\n",
    "    return [None, None]\n",
    "for index, tweet in enumerate(data):\n",
    "\n",
    "    tweet_text = tweet['text']\n",
    "    tweet_text = clean_retweet_text(tweet_text)\n",
    "\n",
    "\n",
    "    possible_presenters  = parse_presenters(tweet_text)\n",
    "    if possible_presenters[0]:\n",
    "      present_resolutions = {\"award\": possible_presenters[0], \"presenters\": possible_presenters[1], \"confidence\": 0.7}\n",
    "      tweet[\"present_resolutions\"] = present_resolutions\n",
    "\n",
    "\n",
    "def get_nominees(text: str):\n",
    "    # Define the regex pattern to match \"nominee/nominated/nominees ... ... best\"\n",
    "    pattern = r\"(.*)\\b(?:nominee|nominated|nominees)\\b(.*?\\bbest\\b.*)\"\n",
    "    # Define a list of exclusion words/phrases\n",
    "    exclusions = r\"\\b(?:not|should've|should have|wasn't|introduce|present|should)\\b\"\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "\n",
    "    # Check each sentence individually\n",
    "    for sentence in sentences:\n",
    "        # Check if any exclusion word is present\n",
    "        if re.search(exclusions, sentence, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "\n",
    "        match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Clean and separate the nominee part and award name\n",
    "            before_nominee = re.sub(r\"\\b(?:nominee|nominated|nominees)\\b\", \"\", match.group(1), flags=re.IGNORECASE).strip()\n",
    "            award_name = re.sub(r\".*?\\bbest\\b\", \"best\", match.group(2), flags=re.IGNORECASE).strip()\n",
    "\n",
    "            return [before_nominee, award_name]\n",
    "\n",
    "\n",
    "    return None\n",
    "def get_nominees2(text: str) :\n",
    "    # Define the regex pattern to match a sentence with a negative word, \"win/won,\" and \"best\"\n",
    "    pattern = r\"(.*?)(\\bdoes not\\b|\\bnot\\b|\\bshould have\\b|\\bshould've\\b|\\bshould\\b|\\bdidn't\\b|\\bdid not\\b)(.*?\\b(?:win|won)\\b.*?\\bbest\\b.*)\"\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.:!?]', text)\n",
    "\n",
    "    # Process each sentence individually\n",
    "    for sentence in sentences:\n",
    "        match = re.search(pattern, sentence.strip(), re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            # Extract the part before the negative word as the nominee\n",
    "            nominee = match.group(1).strip()\n",
    "            # Extract the part from \"best\" onward as the award_name\n",
    "            award_name = re.sub(r\".*?\\bbest\\b\", \"best\", match.group(3), flags=re.IGNORECASE).strip()\n",
    "\n",
    "            # Clean the nominee by removing text before and including \"that\" or \"if\"\n",
    "            nominee = re.sub(r\".*\\b(that|if)\\b\", \"\", nominee, flags=re.IGNORECASE).strip()\n",
    "\n",
    "            return [nominee, award_name]\n",
    "\n",
    "\n",
    "    return None\n",
    "for index, tweet in enumerate(data):\n",
    "\n",
    "    tweet_text = tweet['text']\n",
    "    tweet_text = clean_retweet_text(tweet_text)\n",
    "\n",
    "\n",
    "    possible_nominee  = get_nominees(tweet_text)\n",
    "    if possible_nominee:\n",
    "\n",
    "\n",
    "        nominee_resolutions = {\"award\": possible_nominee[1], \"nominee\": possible_nominee[0], \"confidence\": 0.5}\n",
    "        tweet[\"nominee_resolutions\"] = nominee_resolutions\n",
    "\n",
    "    possible_nominee  = get_nominees2(tweet_text)\n",
    "    if possible_nominee:\n",
    "\n",
    "\n",
    "        nominee_resolutions = {\"award\": possible_nominee[1], \"nominee\": possible_nominee[0], \"confidence\": 0.5}\n",
    "        tweet[\"nominee_resolutions\"] = nominee_resolutions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filename = '_'.join([fname, 'parsed_data.json'])\n",
    "\n",
    "# Write data to JSON file\n",
    "with open(filename, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rYyLHs5ECZm"
   },
   "source": [
    "Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Deqf9giXEF8C"
   },
   "outputs": [],
   "source": [
    "# Open and read extracted tweets\n",
    "with open('_'.join([fname, 'parsed_data.json']), 'r', encoding='utf-8') \\\n",
    "    as extracted_file:\n",
    "    extracted_data = json.load(extracted_file)\n",
    "# Open and read user metadata\n",
    "with open('_'.join([fname, 'user_metadata.json']), 'r', encoding='utf-8') \\\n",
    "    as metadata_file:\n",
    "    user_metadata = json.load(metadata_file)\n",
    "\n",
    "# Find ceremony year\n",
    "dt = datetime.datetime.fromtimestamp(extracted_data[0]['timestamp_ms'] / 1000.0,\n",
    "                                     tz=datetime.timezone.utc)\n",
    "year = dt.year\n",
    "\n",
    "# Import IMDB data relevant to the ceremony year\n",
    "def get_imdb_title_basics():\n",
    "\n",
    "    title_basics = {}\n",
    "\n",
    "    with open('./imdb/title.basics.tsv') as imdb_file:\n",
    "        reader = csv.DictReader(imdb_file, delimiter='\\t', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # Skip title with unknown start years\n",
    "            if row['startYear'] == \"\\\\N\":\n",
    "                continue\n",
    "\n",
    "            # Save title if it is from the current or previous year, OR if it is a\n",
    "            # TV series that ran at some point during that time, AND it is not an\n",
    "            # episode or special\n",
    "            start_year = int(row['startYear'])\n",
    "            if ((row['endYear'] == \"\\\\N\" \\\n",
    "                 and (start_year == year or start_year == year-1 \\\n",
    "                    or (row['titleType'] == \"tvSeries\" and start_year < year))) or \\\n",
    "                (row['endYear'] != \"\\\\N\" and start_year <= year \\\n",
    "                 and int(row['endYear']) >= year-1)) and \\\n",
    "               row['titleType'] != \"tvEpisode\" and \\\n",
    "               row['titleType'] != 'tvSpecial':\n",
    "                category = row['titleType'].lower()\n",
    "                if category not in title_basics:\n",
    "                    title_basics[category] = []\n",
    "                title = row['primaryTitle']\n",
    "                title_basics[category].append(title)\n",
    "\n",
    "    return title_basics\n",
    "\n",
    "title_basics = get_imdb_title_basics()\n",
    "# IMDB genre tags, minus 'Short' and 'Adult'\n",
    "genres = ['Comedy', 'Music', 'Crime', 'Drama', 'Game-Show', 'Talk-Show', 'Family', 'Mystery', 'Sport', 'Horror', 'Western', 'Adventure', 'News', 'Action', 'Documentary', 'Reality-TV', 'Sci-Fi', 'Thriller', 'Animation', 'War', 'Musical', 'Romance', 'Fantasy', 'Biography', 'History']\n",
    "# Set up results object\n",
    "results = {}\n",
    "\n",
    "# Ranking\n",
    "possible_hosts = Counter()\n",
    "possible_awards = Counter()\n",
    "for tweet in extracted_data:\n",
    "    # Calculate tweet weight based on user + tweet reliability\n",
    "    user = tweet['user']['screen_name'].lower()\n",
    "    tweet['weight'] = (tweet['retweets'] + 1) \\\n",
    "        + (user_metadata[user]['rt_average'] + 1)\n",
    "\n",
    "    # Host\n",
    "    if 'host_resolutions' in tweet:\n",
    "        hr = tweet['host_resolutions']\n",
    "        for host in hr['hosts']:\n",
    "            possible_hosts[host.lower()] += tweet['weight'] * hr['confidence']\n",
    "\n",
    "    # Award\n",
    "    if 'win_resolutions' in tweet:\n",
    "        wr = tweet['win_resolutions']\n",
    "        for award in wr['award']:\n",
    "            possible_awards[award.lower()] += tweet['weight'] * wr['confidence']\n",
    "    if 'nominee_resolutions' in tweet:\n",
    "        nr = tweet['nominee_resolutions']\n",
    "        possible_awards[nr['award'].lower()] += tweet['weight'] * nr['confidence']\n",
    "    if 'present_resolutions' in tweet:\n",
    "        pr = tweet['present_resolutions']\n",
    "        possible_awards[pr['award'].lower()] += tweet['weight'] * pr['confidence']\n",
    "\n",
    "\n",
    "# Find closest match by averaging distance calculations\n",
    "def closest_match(names, possible_names, diff):\n",
    "    if not isinstance(names, list):\n",
    "        names = [names]\n",
    "\n",
    "    max_ratio = []\n",
    "    for name in names:\n",
    "        name = name.lower()\n",
    "        sm = difflib.SequenceMatcher(b=name)\n",
    "        for other_name in possible_names:\n",
    "            other_name = other_name.lower()\n",
    "            ratio = 1 - distance.edit_distance(name, other_name) / \\\n",
    "                max(len(name), len(other_name))\n",
    "            sm.set_seq1(other_name)\n",
    "            ratio = (sm.ratio() + ratio)/2.0\n",
    "            if not max_ratio or (max_ratio and ratio > max_ratio[1]):\n",
    "                max_ratio = [other_name, ratio]\n",
    "\n",
    "    if max_ratio and max_ratio[1] > diff:\n",
    "        return max_ratio[0]\n",
    "    return []\n",
    "\n",
    "# Combine like name categories\n",
    "def combine_names(name, possible_names, orig_vals, len_penalty, diff):\n",
    "    if name not in possible_names:\n",
    "         return\n",
    "\n",
    "    # Temporarily remove current name to avoid comparing it to itself\n",
    "    temp = possible_names.pop(name)\n",
    "\n",
    "    # Check for direct substrings\n",
    "    for other_name in possible_names:\n",
    "        # Prioritize longer string, unless large popularity difference\n",
    "        if (name in other_name \\\n",
    "            and orig_vals[name] * len_penalty < orig_vals[other_name]) \\\n",
    "                or (other_name in name \\\n",
    "                    and orig_vals[other_name] * len_penalty >= orig_vals[name]):\n",
    "                possible_names[other_name] += temp\n",
    "                return\n",
    "        elif name in other_name or other_name in name:\n",
    "            possible_names[name] = temp + possible_names[other_name]\n",
    "            del possible_names[other_name]\n",
    "            return\n",
    "\n",
    "    # If not a direct substring, check for misspellings\n",
    "    closest = closest_match(name, possible_names.keys(), diff)\n",
    "    if closest and orig_vals[name] > orig_vals[closest]:\n",
    "        possible_names[name] = temp + possible_names[closest]\n",
    "        del possible_names[closest]\n",
    "        return\n",
    "    elif closest:\n",
    "        possible_names[closest] += temp\n",
    "        return\n",
    "\n",
    "    # Reinsert current name if it is distinct\n",
    "    possible_names[name] = temp\n",
    "\n",
    "# Retrieve the most likely combination of names from a given set\n",
    "def get_names(possible_names, threshold):\n",
    "    names = []\n",
    "    if not possible_names:\n",
    "        return names\n",
    "\n",
    "    # Set the minimum score based on the given threshold\n",
    "    min_score = threshold\n",
    "    if threshold <= 1:\n",
    "        min_score *= possible_names[0][1]\n",
    "    for name in possible_names:\n",
    "        # Insert all possible hosts above the minimum score\n",
    "        if name[1] >= min_score:\n",
    "            names.append(name[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return names\n",
    "\n",
    "# Aggregate results for a category\n",
    "def aggregate_results(possible_options, len_penalty, diff, threshold):\n",
    "    in_order = possible_options.most_common()\n",
    "    in_order.reverse()\n",
    "    orig_vals = dict(in_order)\n",
    "    for name in orig_vals:\n",
    "        combine_names(name, possible_options, orig_vals, len_penalty, diff)\n",
    "    return get_names(possible_options.most_common(), threshold)\n",
    "\n",
    "## Aggregation\n",
    "\n",
    "# Hosts\n",
    "results['hosts'] = aggregate_results(possible_hosts, 0.8, 0.8, 0.8)\n",
    "\n",
    "# Award names\n",
    "awards = aggregate_results(possible_awards, 0.8, 0.7, 10)\n",
    "for award in awards:\n",
    "    results[award] = {}\n",
    "\n",
    "# Per-award ranking\n",
    "possible_winners = {}\n",
    "possible_nominees = {}\n",
    "possible_presenters = {}\n",
    "\n",
    "for award in awards:\n",
    "    possible_winners[award] = Counter()\n",
    "    possible_nominees[award] = Counter()\n",
    "    possible_presenters[award] = Counter()\n",
    "\n",
    "for tweet in extracted_data:\n",
    "\n",
    "    # Winner\n",
    "    if 'win_resolutions' in tweet:\n",
    "        wr = tweet['win_resolutions']\n",
    "        award = closest_match(wr['award'], awards, 0.8)\n",
    "        if award:\n",
    "            for winner in wr['winner']:\n",
    "                if winner:\n",
    "                    possible_winners[award][winner.lower()] += \\\n",
    "                        tweet['weight'] * wr['confidence']\n",
    "\n",
    "    # Nominee\n",
    "    if 'nominee_resolutions' in tweet:\n",
    "        nr = tweet['nominee_resolutions']\n",
    "        award = closest_match(nr['award'], awards, 0.8)\n",
    "        if award and nr['nominee']:\n",
    "            possible_nominees[award][nr['nominee'].lower()] += \\\n",
    "                tweet['weight'] * nr['confidence']\n",
    "\n",
    "    # Presenter\n",
    "    if 'present_resolutions' in tweet:\n",
    "        pr = tweet['present_resolutions']\n",
    "        award = closest_match(pr['award'], awards, 0.8)\n",
    "        if award:\n",
    "            for presenter in pr['presenters']:\n",
    "                if presenter:\n",
    "                    possible_presenters[award][presenter.lower()] += \\\n",
    "                        tweet['weight'] * pr['confidence']\n",
    "\n",
    "# Per-award aggregation\n",
    "win_penalty = 0.1\n",
    "for award in awards:\n",
    "    winner = aggregate_results(possible_winners[award], 0.8, 0.8, 1)\n",
    "    if winner:\n",
    "        results[award]['winner'] = winner[0]\n",
    "        # Insert award winner into possible nominees\n",
    "        possible_nominees[award][winner[0]] = \\\n",
    "            win_penalty * possible_winners[award][winner[0]]\n",
    "    else:\n",
    "        results[award]['winner'] = ''\n",
    "    results[award]['nominees'] = \\\n",
    "        aggregate_results(possible_nominees[award], 0.8, 0.8, 0.2)\n",
    "    results[award]['presenters'] = \\\n",
    "        aggregate_results(possible_presenters[award], 0.8, 0.8, 0.4)\n",
    "\n",
    "######Humor: Addtional goal\n",
    "humor = ['guffaw', 'fun', 'smirk', 'yelp', 'wag', 'lol', 'funny', 'wry', 'sneer', 'joke']\n",
    "#store data in list\n",
    "humor_list = []\n",
    "for t in extracted_data:\n",
    "    for x in humor:\n",
    "        if x in t['text']:\n",
    "            humor_list.append((t['user']['screen_name'], t['text']))\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# Output to JSON\n",
    "with open('_'.join([fname, 'results.json']), 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(results, output_file, indent=6)\n",
    "# Output human-readable to TXT\n",
    "with open('_'.join([fname, 'results.txt']), 'w', encoding='utf-8') as output_file:\n",
    "    print('Hosts:', (', '.join(results['hosts']).title()), file=output_file)\n",
    "    for award in awards:\n",
    "        print(\"\\nAward: \", award.title(), sep='', file=output_file)\n",
    "        for category in results[award]:\n",
    "            if isinstance(results[award][category], list):\n",
    "                print(category.title(), \": \",\n",
    "                      (', '.join(results[award][category])).title(), sep='',\n",
    "                      file=output_file)\n",
    "            else:\n",
    "                print(category.title(), \": \", results[award][category].title(),\n",
    "                      sep='', file=output_file)\n",
    "    print()\n",
    "    for y in humor_list:\n",
    "        print(f\"Humor_Who: {y[0]}, Best Jokes: {y[1]}\", file=output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
